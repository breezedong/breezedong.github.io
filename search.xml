<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Predicting Vulnerable Software Components via Text Mining]]></title>
      <url>http://yoursite.com/2016/06/01/Mining/</url>
      <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>文章于2014年10月份发表在IEEE TRANSACTIONS ON SOFTWARE ENGINEERING，一篇通过文本挖掘技术进行软件漏洞检测的论文。文章本身引用只有<a href="https://scholar.google.com/scholar?hl=zh-CN&amp;q=Predicting+Vulnerable+Software+Components+via+Text+Mining&amp;btnG=&amp;lr=" target="_blank" rel="external">14</a>，创新点也不是很新，但由于其期刊等级较高，而且文章数据处理分析较多，还是值得以后写作借鉴。</p>
<ul>
<li>出处：IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 40, NO. 10, OCTOBER 2014</li>
<li>作者：Riccardo Scandariato, James Walden, Aram Hovsepyan, and Wouter Joosen</li>
</ul>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="论文主要观点"><a href="#论文主要观点" class="headerlink" title="论文主要观点"></a>论文主要观点</h2><p>将Android app应用软件源代码视作文本，源代码中语句与词类比于文本中词，作为数据特征，使用朴素贝叶斯和随机森林的算法，构建软件源代码漏洞预测模型。</p>
<h2 id="成果"><a href="#成果" class="headerlink" title="成果"></a>成果</h2><ul>
<li>首次将文本挖掘相关方法应用于软件漏洞预测，直接使用源代码而非软件语义、开发者相关特征作为特征进行预测。</li>
<li>预测模型相对于当前的软件漏洞预测模型，具有更好的准确性和召回率。</li>
</ul>
<h1 id="方法模型"><a href="#方法模型" class="headerlink" title="方法模型"></a>方法模型</h1><h2 id="相关工作图"><a href="#相关工作图" class="headerlink" title="相关工作图"></a>相关工作图</h2><p>作者使用五个维度的信息来评价对比软件漏洞预测模型相关工作，如下图所示：<br><img src="https://git.oschina.net/breezedong/figure_bed/raw/master/vulnerability_prediction_models_related_work.jpg" alt="image"></p>
<h2 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h2><ul>
<li>样本选择selection of applications：<ul>
<li>source: the F-Droid repository (f-droid.org) </li>
<li>selection criteria:programming language, application size, and the number of<br>versions released</li>
</ul>
</li>
<li>漏洞数据构建construction of dataset：<ul>
<li>tool:HP Fortify SCA  scan the source code to present vulnerablity warnings of the applications</li>
<li>why:too few vulnerabilities(NVD<br>(nvd.nist.gov)) related to Android applications </li>
</ul>
</li>
<li>输入构建input：Each Java file is tokenized into a vector of terms</li>
<li>机器学习方法选择machine learning techniques:five, wellknown learning techniques are applied to the approache: Decision Trees, k-Nearest<br>Neighbor, Na€ ıve Bayes, Random Forest and support vector machine (SVM). Best results are obtained with <strong>NB and Random Forest</strong>.</li>
<li>实验设计experiments design:<ul>
<li>验证方法validation:10-fold cross-validation</li>
<li>experiment 1:built models with both Na€ ıve Bayes and Random Forest machine learning techniques based on the first version (v0) of each application.prove the method can be used to build high quality prediction models for Android applications.</li>
<li>experiment 2:built a prediction model based on the initial version (using all source files available in v0) and predicted all subsequent versions of that application (v1 andfollowing) prediction technique can forecast with excellent performance the vulnerable files of the future versions of an Android application</li>
<li>experiment 3：built 20 models using version v0 of each application. We then tested each model by predicting vulnerable files in the v0 versions of the other 19 applications.a single application can predict which software components are vulnerable in other applications</li>
</ul>
</li>
</ul>
<h1 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h1><ul>
<li>文本挖掘方法应用于软件缺陷检测</li>
<li>实验设计上，进行三个方向上的对比实验</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>文章的实验部分数据对比写的不错，很简单的创意和想法，做出了三组实验</li>
</ul>
<h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ul>
<li>创新点较为简单</li>
<li>数据对比牵强，数据集不同</li>
<li>期刊论文过于滞后，本文2014年10月发表，但是研究时间节点在2012年前后，以后尽量多看会议论文，期刊论文仅作为参考文献</li>
</ul>
<h2 id="我的想法"><a href="#我的想法" class="headerlink" title="我的想法"></a>我的想法</h2><ul>
<li>结合vccfinder论文</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[数据结构与算法]]></title>
      <url>http://yoursite.com/2016/05/29/data-algorithm/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote>
<p>近日压力倍增，在图书馆自习，期望能够看更多的论文。休息期间找到一本算法的教材，从机器学习和人工智能的角度重温了一遍数据结构与算法。</p>
</blockquote>
<p>本科阶段曾学习过<strong>数据结构与算法</strong>这门课程，使用的是C语言实现的小绿书。依稀记得当时老师只是讲解了计算机程序中常见的数据结构，<strong>例如线性表，队列，栈等的存储方式以及增删查改的操作实现方法</strong>，并且给出了程序时间复杂度和空间复杂度的概念和分析方法。<br>很明确的一个概念：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">program = data + algorithm</span><br></pre></td></tr></table></figure>
<h1 id="数据结构算法与机器学习"><a href="#数据结构算法与机器学习" class="headerlink" title="数据结构算法与机器学习"></a>数据结构算法与机器学习</h1><p>最近研究了很多机器学习相关的概念和算法，发现其的核心理念就是使用计算机模拟人类的学习方法，来处理数据。才发现其实数据结构与算法是机器学习的基础。<br>二者都是使用算法来处理数据，得到一些结果。<br>区别在于，机器学习的数据非此处的结构化数据，往往需要进行预处理和特征抽取；算法也是复杂得多的模型和训练算法，目的是为了进行预测。<br>总体来说，计算机的数据结构与算法，是机器学习的基础。</p>
<h1 id="数据结构与算法的架构"><a href="#数据结构与算法的架构" class="headerlink" title="数据结构与算法的架构"></a>数据结构与算法的架构</h1><p>本书将数据结构与算法分为八大思想，数据结构，数据查找和排序四大部分。其中，八大思想讲述了日常算法中常用的八种思想；数据结构体现了数据在计算机中的组织形式，包括物理形式和逻辑形式，给出了数据在计算机中存储和运算的基本方法；最后，查找和排序，是数据处理最常见的需求，也是最基本的算法。<br>组织形式如下：</p>
<ul>
<li>算法思想<ul>
<li>枚举</li>
<li>递归</li>
<li>递推</li>
<li>迭代</li>
<li>分治</li>
<li>贪心</li>
<li>试探</li>
<li>模拟</li>
</ul>
</li>
<li>数据结构<ul>
<li>基本结构<ul>
<li>线性表</li>
<li>队列</li>
<li>栈</li>
</ul>
</li>
<li>逻辑结构<ul>
<li>树<ul>
<li>二叉树</li>
<li>霍夫曼树</li>
</ul>
</li>
<li>图<ul>
<li>有向图</li>
<li>无向图</li>
<li>连通图</li>
<li>生成树</li>
<li>深度遍历</li>
<li>广度遍历</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>查找<ul>
<li>基于线性表的查找</li>
<li>基于树的查找</li>
</ul>
</li>
<li>排序(<a href="http://www.cricode.com/3212.html" target="_blank" rel="external">图文详解八大排序算法</a>)<ul>
<li>交换排序<ul>
<li>冒泡排序</li>
<li>快速排序</li>
</ul>
</li>
<li>插入排序<ul>
<li>希尔排序</li>
</ul>
</li>
<li>选择排序<ul>
<li>堆排序</li>
</ul>
</li>
<li>归并排序</li>
</ul>
</li>
</ul>
<p>相关思维导图如下图：<br><img src="https://git.oschina.net/breezedong/figure_bed/raw/master/data_algorithm.png" alt="image"></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[深度学习轶事]]></title>
      <url>http://yoursite.com/2016/05/25/deeplearning%E8%BD%B6%E4%BA%8B/</url>
      <content type="html"><![CDATA[<p>最近在研究深度学习，在阅读文献过程中，也接触到不少名人轶事，个人认为非常有意思，对理解整个人工智能发展历程有不少帮助。</p>
<h2 id="Geoffrey-Hinton-amp-AI-revolution"><a href="#Geoffrey-Hinton-amp-AI-revolution" class="headerlink" title="Geoffrey Hinton &amp; AI revolution"></a>Geoffrey Hinton &amp; AI revolution</h2><blockquote>
<p>从1950年图灵提出图灵测试开始，人们对人工智能有了很高的期待。然而，半个多世纪过去，人工智能除了能用一些神经网络、支持向量机等浅层学习算法拟合一些线性模型和少数简单非线性模型外，远远没有达到图灵测试的要求，甚至有人讲机器学习称为伪科学。直到Geoffrey Hinton在2006年提出深度学习，这一能够模拟人类处理“抽象概念”的算法模型…</p>
</blockquote>
<p>图灵测试：<br>隔墙对话，让人分不出对方是机器还是人类。<br>人工智能发展半个多世纪，离图灵测试相距甚远，直到深度学习的出现。<br>其基于抽象概念的深层次网络，丰富的参数，能够拟合复杂的非线性函数与模型，对大规模数据具有良好的表达处理能力。<br>这里不得不提的人物： </p>
<ul>
<li>Geoffrey Hinton：神经网络鼻祖，深度学习发明者，曾任教Cambridge、CMU，目前任教University of Toronto</li>
<li>Yoshua Bengio：经历比较简单，McGill University 获得博士后，去 MIT 追随 Mike Jordan 做博士后。目前任教 University of Montreal。</li>
</ul>
<h2 id="Google-兵分两路"><a href="#Google-兵分两路" class="headerlink" title="Google 兵分两路"></a>Google 兵分两路</h2><p>Deep Learning引爆的这一波人工智能热潮，不仅在学术界，更在工业界产生不小的影响。相关的技术，在工业界犹如一把开山斧，等待着是各大公司一路披荆斩棘，动用资本和商业的强力手段，跑马圈地了。<br>Google作为全球从学术界转入工业界的翘楚与典范，自然不会错过这次机会。<br>Google兵分两路，左路以 Jeff Dean 和 Andrew Ng 为首，重点突破 Deep Learning等等算法和应用，右路军由 Amit Singhal 领军，目标是构建 Knowledge Graph 基础设施。</p>
<p>不得不提的大牛：</p>
<ul>
<li>Jeff Dean：  Google 诸位 Fellows 中，名列榜首，GFS 就是他的杰作。Andrew Ng 本科时，就读 CMU，后来去 MIT 追随 Mike Jordan。</li>
<li>Andrew Ng：Mike Jordan 在 MIT 人缘不好，后来愤然出走 UC Berkeley。Andrew Ng 毫不犹豫追随导师，也去了 Berkeley。拿到博士后，任教 Stanford，是 Stanford 新生代教授中的佼佼者，同时兼职 Google。</li>
<li>Amit Singhal：1996 年 Amit Singhal 从 Cornell University 拿到博士学位后，去 Bell Lab 工作，2000 年加盟 Google。据说他去 Google 面试时，对 Google 创始人 Sergey Brian 说，“Your engine is excellent, but let me rewirte it!”。Amit Singhal 目前任职 Google 高级副总裁，掌管 Google最核心的业务，搜索引擎。</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[UFLDL深度学习笔记]]></title>
      <url>http://yoursite.com/2016/05/25/DeepLearningnotes/</url>
      <content type="html"><![CDATA[<h1 id="深度学习学习笔记"><a href="#深度学习学习笔记" class="headerlink" title="深度学习学习笔记"></a>深度学习学习笔记</h1><blockquote>
<p>最近研究一篇论文，使用RNN（recurrence neural networks）识别二进制代码中函数的方法，考虑到相关的CNN，LSTM，t-SNE，LDA等名词和相关算法一直碰到，避无可避，遂整体的学习了一下DeepLearning相关的知识。</p>
</blockquote>
<h2 id="UFLDL教程"><a href="#UFLDL教程" class="headerlink" title="UFLDL教程"></a>UFLDL教程</h2><p>学习了<a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B" target="_blank" rel="external">UFLDL教程</a>，理解深度学习相关的概念与思想，主要包括基本概念、数据处理以及深度学习三个部分，并画出思维导图如下：</p>
<ul>
<li>基本概念：主要包含参数，激励函数，代价函数等</li>
<li>相关数据处理：PCA，whitening，样本的均值与方差等</li>
<li>深度学习：深层神经网络，根据特性，有CNN,RNN等</li>
</ul>
<p><img src="https://git.oschina.net/breezedong/figure_bed/raw/master/ANN.png" alt="image"></p>
<p>从上面教程中，我学习到了神经网络的模型的基本概念、模型参数以及训练方法：<strong>通过初始化参数，进行前向传递，得到初始输出结果，根据初始输出与标准输出构造代价函数，通过梯度下降与反向传播求导的方法，调整参数，迭代直至参数最优</strong><br>教程主要侧重神经网络的数学求解。</p>
<h2 id="zouxy09深度学习笔记整理系列博客"><a href="#zouxy09深度学习笔记整理系列博客" class="headerlink" title="zouxy09深度学习笔记整理系列博客"></a>zouxy09深度学习笔记整理系列博客</h2><p>而后，学习<a href="http://blog.csdn.net/zouxy09/article/details/8775488" target="_blank" rel="external">zouxy09</a>深度学习笔记整理系列博客，从物理意义上去理解深度学习。</p>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>机器学习的目的就是使计算机模拟人类的学习方式，获得知识与技能，解决问题。<br>机器学习在很多领域都有着应用：</p>
<ul>
<li>image: computer vision</li>
<li>voice: speech recognition</li>
<li>text: natural language process</li>
</ul>
<p>他们当前都有着很类似的处理过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">low-level-sensing--&gt;pre-processing</span><br><span class="line"></span><br><span class="line">pre-processing--&gt;feature-extract</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">feature-extract--&gt;feature-selection</span><br><span class="line"></span><br><span class="line">feature-selection--&gt;inference-prediction-recognition</span><br></pre></td></tr></table></figure>
<p>即：通过传感器收集数据，经过数据预处理，得到可输入数据，经过特征工程，对数据特征进行抽取，选择，得到表征特点的特征，输入系统，算法进行推理，预测和识别等任务。</p>
<p>当前的机器学习，虽然在监督学习方法上，取得了很好的成果，但学习前往往需要在数据预处理、特征抽取和特征选择三个步骤耗费大量的人工精力。人工特征工程，是一件非常耗时耗力、启发式（需要专业的知识）的方法，选取的结果在一定程度上依赖经验和运气。</p>
<p>在此背景下，神经网络结合人脑视觉原理，将神经网络与层层抽象，形成深层神经网络；加上卷积等局部连接的方法，使得深层神经网络的参数训练得以实现。深度学习最大的优势在于，它可以让机器来学习特征，从而解决前面的特征工程问题。<strong>深度学习亦可称为unsupervised feature learning</strong>.</p>
<h3 id="特征表示"><a href="#特征表示" class="headerlink" title="特征表示"></a>特征表示</h3><p>1995 年前后，Bruno Olshausen和 David Field 两位学者做了一个实验。<br>收集很多黑白风景照，抽取了400张16*16的碎片：S[i]i=0,…,399，然后随意的抽取了一张碎片T。<br>通过叠加的方法，抽取一组S[i]中碎片拟合T。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum(a[k]*S[k]) -&gt; T</span><br></pre></td></tr></table></figure>
<p>其中，a[k]为叠加权重。<br>为解决这个问题，Bruno Olshausen和 David Field 发明了一个算法，稀疏编码（Sparse Coding）。稀疏编码是一个重复迭代的过程，每次迭代分两步：</p>
<ol>
<li><p>选择一组 S[k]，然后调整 a[k]，使得Sum_k (a[k] * S[k]) 最接近 T。</p>
</li>
<li><p>固定住 a[k]，在 400个碎片中，选择其它更合适的碎片S’[k]，替代原先的 S[k]，使得Sum_k (a[k] * S’[k]) 最接近 T。</p>
</li>
</ol>
<p>经过几次迭代后，最佳的 S[k] 组合，被遴选出来了。令人惊奇的是，被选中的 S[k]，基本上都是照片上不同物体的边缘线，这些线段形状相似，区别在于方向。<br><strong>也就是说，复杂图形，往往由一些基本结构组成<br>。</strong><br>如下图所示：<img src="https://git.oschina.net/breezedong/figure_bed/raw/master/dl-feature-combine.jpg" alt="image"></p>
<p> 小块的图形可以由基本edge构成，更结构化，更复杂的，具有概念性的图形如何表示呢？这就需要更高层次的特征表示，比如V2，V4。因此V1看像素级是像素级。V2看V1是像素级，这个是层次递进的，高层表达由底层表达的组合而成。专业点说就是基basis。V1取提出的basis是边缘，然后V2层是V1层这些basis的组合，这时候V2区得到的又是高一层的basis。即上一层的basis组合的结果，上上层又是上一层的组合basis……（所以有大牛说Deep learning就是“搞基”，因为难听，所以美其名曰Deep learning或者Unsupervised Feature Learning）</p>
<p>从文本来说，一个doc表示什么意思？我们描述一件事情，用什么来表示比较合适？用一个一个字嘛，我看不是，字就是像素级别了，起码应该是term，换句话说每个doc都由term构成，但这样表示概念的能力就够了嘛，可能也不够，需要再上一步，达到topic级，有了topic，再到doc就合理。但每个层次的数量差距很大，比如doc表示的概念-&gt;topic（千-万量级）-&gt;term（10万量级）-&gt;word（百万量级）。</p>
<p>一个人在看一个doc的时候，眼睛看到的是word，由这些word在大脑里自动切词形成term，在按照概念组织的方式，先验的学习，得到topic，然后再进行高层次的learning。 </p>
<h3 id="浅层学习与深度学习"><a href="#浅层学习与深度学习" class="headerlink" title="浅层学习与深度学习"></a>浅层学习与深度学习</h3><h4 id="Shallow-Learning"><a href="#Shallow-Learning" class="headerlink" title="Shallow Learning"></a>Shallow Learning</h4><p>浅层学习是机器学习的第一次浪潮。</p>
<p>20世纪80年代末期，用于人工神经网络的反向传播算法（也叫Back Propagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基于统计模型的机器学习热潮。这个热潮一直持续到今天。人们发现，利用BP算法可以让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件做预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显出优越性。这个时候的人工神经网络，虽也被称作多层感知机（Multi-layer Perceptron），但实际是种只含有一层隐层节点的浅层模型。</p>
<p>20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、 Boosting、最大熵方法（如LR，Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂。</p>
<h4 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h4><p>深度学习是机器学习的第二次浪潮。</p>
<p> 2006年，加拿大多伦多大学教授、机器学习领域的泰斗Geoffrey Hinton和他的学生RuslanSalakhutdinov在《科学》上发表了一篇文章，开启了深度学习在学术界和工业界的浪潮。这篇文章有两个主要观点：1）多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2）深度神经网络在训练上的难度，可以通过“逐层初始化”（layer-wise pre-training）来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。</p>
<p>当前多数分类、回归等学习方法为浅层结构算法，其局限性在于有限样本和计算单元情况下对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受到一定制约。深度学习可通过学习一种深层非线性网络结构，实现复杂函数逼近，表征输入数据分布式表示，并展现了强大的从少数样本集中学习数据集本质特征的能力。（多层的好处是可以用较少的参数表示复杂的函数）</p>
<p>深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。</p>
<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p>卷积神经网络讲解：<br><a href="http://yann.lecun.com/exdb/lenet/index.html" target="_blank" rel="external">zouxy09博客</a></p>
<p>经典案例：<br><a href="http://yann.lecun.com/exdb/lenet/index.html" target="_blank" rel="external">手写识别</a></p>
<h3 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h3><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>深度学习是将特征工程与分类预测任务结合，自动学习特征，由低层次特征向高层次特征不断抽象，形成多层次神经网络的模型方法。<br>其最主要特征在于：<strong>其深度的层次，丰富的参数赋予其强大的表达能力，能够拟合更复杂的函数与模型，特别是非线性函数，对大规模数据分析处理上较浅层算法有良好的效果。</strong></p>
<h4 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h4><p>深度学习作为当前监督学习算法中，在大规模数据背景下最优秀的算法，在computer vision/speech recognition/natural language process等方面有着很好的应用。</p>
<ol>
<li>从应用方面：</li>
</ol>
<ul>
<li>其在不同的应用场景下，学习框架的设计，是非常复杂的，需要更多的理论支撑和实际的测试。</li>
<li>深度学习参数众多，实际使用过程中，参数的设计与训练是难点。</li>
<li>怎样结合并行计算的优势，来构建深度学习。</li>
</ul>
<ol>
<li><p>从学习方法方面：<br>深度学习在监督学习方面应用做的很好，但在计算机模拟人类学习的需求下，非监督学习在人类学习方法中占主要比例。这也是深度学习的发展方向。</p>
</li>
<li><p>从实际使用方面：<br>据说hinton都说不清深度学习是怎么工作的，估计世界上很少有人能说出为什么它里面的物理意义吧。当前最好的办法就是，把dl当做黑箱子来用，基于经验和结果来调整参数。</p>
</li>
</ol>
]]></content>
    </entry>
    
  
  
</search>
